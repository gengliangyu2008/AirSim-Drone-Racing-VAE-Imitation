import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization, Lambda, Concatenate, Conv2DTranspose, Reshape, ReLU
from tensorflow.python.keras.backend import epsilon

'''
This code is from below link:
https://towardsdatascience.com/implementing-capsule-network-in-tensorflow-11e4cca5ecae
'''
class CapsuleNetwork(tf.keras.Model):
    def __init__(self, no_of_conv_kernels, no_of_primary_capsules, primary_capsule_vector, no_of_secondary_capsules,
                 secondary_capsule_vector, r):
        super(CapsuleNetwork, self).__init__()
        self.no_of_conv_kernels = no_of_conv_kernels
        self.no_of_primary_capsules = no_of_primary_capsules
        self.primary_capsule_vector = primary_capsule_vector
        self.no_of_secondary_capsules = no_of_secondary_capsules
        self.secondary_capsule_vector = secondary_capsule_vector
        self.r = r

        with tf.name_scope("Variables") as scope:
            self.convolution = tf.keras.layers.Conv2D(self.no_of_conv_kernels, [9, 9], strides=[1, 1],
                                                      name='ConvolutionLayer', activation='relu')
            self.primary_capsule = tf.keras.layers.Conv2D(self.no_of_primary_capsules * self.primary_capsule_vector,
                                                          [9, 9], strides=[2, 2], name="PrimaryCapsule")
            self.w = tf.Variable(tf.random_normal_initializer()(
                shape=[1, 1152, self.no_of_secondary_capsules, self.secondary_capsule_vector,
                       self.primary_capsule_vector]), dtype=tf.float32, name="PoseEstimation", trainable=True)
            self.dense_1 = tf.keras.layers.Dense(units=512, activation='relu')
            self.dense_2 = tf.keras.layers.Dense(units=1024, activation='relu')
            self.dense_3 = tf.keras.layers.Dense(units=784, activation='sigmoid', dtype='float32')

    def build(self, input_shape):
        pass

    def squash(self, s):
        with tf.name_scope("SquashFunction") as scope:
            s_norm = tf.norm(s, axis=-1, keepdims=True)
            return tf.square(s_norm) / (1 + tf.square(s_norm)) * s / (s_norm + epsilon)

    @tf.function
    def call(self, inputs):
        input_x, y = inputs
        # input_x.shape: (None, 28, 28, 1)
        # y.shape: (None, 10)

        x = self.convolution(input_x)  # x.shape: (None, 20, 20, 256)
        x = self.primary_capsule(x)  # x.shape: (None, 6, 6, 256)

        with tf.name_scope("CapsuleFormation") as scope:
            u = tf.reshape(x,
                           (-1, self.no_of_primary_capsules * x.shape[1] * x.shape[2], 8))  # u.shape: (None, 1152, 8)
            u = tf.expand_dims(u, axis=-2)  # u.shape: (None, 1152, 1, 8)
            u = tf.expand_dims(u, axis=-1)  # u.shape: (None, 1152, 1, 8, 1)
            u_hat = tf.matmul(self.w, u)  # u_hat.shape: (None, 1152, 10, 16, 1)
            u_hat = tf.squeeze(u_hat, [4])  # u_hat.shape: (None, 1152, 10, 16)

        with tf.name_scope("DynamicRouting") as scope:
            b = tf.zeros((input_x.shape[0], 1152, self.no_of_secondary_capsules, 1))  # b.shape: (None, 1152, 10, 1)
            for i in range(self.r):  # self.r = 3
                c = tf.nn.softmax(b, axis=-2)  # c.shape: (None, 1152, 10, 1)
                s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1, keepdims=True)  # s.shape: (None, 1, 10, 16)
                v = self.squash(s)  # v.shape: (None, 1, 10, 16)
                agreement = tf.squeeze(
                    tf.matmul(tf.expand_dims(u_hat, axis=-1), tf.expand_dims(v, axis=-1), transpose_a=True),
                    [4])  # agreement.shape: (None, 1152, 10, 1)
                # Before matmul following intermediate shapes are present, they are not assigned to a variable but just for understanding the code.
                # u_hat.shape (Intermediate shape) : (None, 1152, 10, 16, 1)
                # v.shape (Intermediate shape): (None, 1, 10, 16, 1)
                # Since the first parameter of matmul is to be transposed its shape becomes:(None, 1152, 10, 1, 16)
                # Now matmul is performed in the last two dimensions, and others are broadcasted
                # Before squeezing we have an intermediate shape of (None, 1152, 10, 1, 1)
                b += agreement

        with tf.name_scope("Masking") as scope:
            y = tf.expand_dims(y, axis=-1)  # y.shape: (None, 10, 1)
            y = tf.expand_dims(y, axis=1)  # y.shape: (None, 1, 10, 1)
            mask = tf.cast(y, dtype=tf.float32)  # mask.shape: (None, 1, 10, 1)
            v_masked = tf.multiply(mask, v)  # v_masked.shape: (None, 1, 10, 16)

        with tf.name_scope("Reconstruction") as scope:
            v_ = tf.reshape(v_masked, [-1,
                                       self.no_of_secondary_capsules * self.secondary_capsule_vector])  # v_.shape: (None, 160)
            reconstructed_image = self.dense_1(v_)  # reconstructed_image.shape: (None, 512)
            reconstructed_image = self.dense_2(reconstructed_image)  # reconstructed_image.shape: (None, 1024)
            reconstructed_image = self.dense_3(reconstructed_image)  # reconstructed_image.shape: (None, 784)

        return v, reconstructed_image

    @tf.function
    def predict_capsule_output(self, inputs):
        x = self.convolution(inputs)  # x.shape: (None, 20, 20, 256)
        x = self.primary_capsule(x)  # x.shape: (None, 6, 6, 256)

        with tf.name_scope("CapsuleFormation") as scope:
            u = tf.reshape(x,
                           (-1, self.no_of_primary_capsules * x.shape[1] * x.shape[2], 8))  # u.shape: (None, 1152, 8)
            u = tf.expand_dims(u, axis=-2)  # u.shape: (None, 1152, 1, 8)
            u = tf.expand_dims(u, axis=-1)  # u.shape: (None, 1152, 1, 8, 1)
            u_hat = tf.matmul(self.w, u)  # u_hat.shape: (None, 1152, 10, 16, 1)
            u_hat = tf.squeeze(u_hat, [4])  # u_hat.shape: (None, 1152, 10, 16)

        with tf.name_scope("DynamicRouting") as scope:
            b = tf.zeros((inputs.shape[0], 1152, self.no_of_secondary_capsules, 1))  # b.shape: (None, 1152, 10, 1)
            for i in range(self.r):  # self.r = 3
                c = tf.nn.softmax(b, axis=-2)  # c.shape: (None, 1152, 10, 1)
                s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1, keepdims=True)  # s.shape: (None, 1, 10, 16)
                v = self.squash(s)  # v.shape: (None, 1, 10, 16)
                agreement = tf.squeeze(
                    tf.matmul(tf.expand_dims(u_hat, axis=-1), tf.expand_dims(v, axis=-1), transpose_a=True),
                    [4])  # agreement.shape: (None, 1152, 10, 1)
                # Before matmul following intermediate shapes are present, they are not assigned to a variable but just for understanding the code.
                # u_hat.shape (Intermediate shape) : (None, 1152, 10, 16, 1)
                # v.shape (Intermediate shape): (None, 1, 10, 16, 1)
                # Since the first parameter of matmul is to be transposed its shape becomes:(None, 1152, 10, 1, 16)
                # Now matmul is performed in the last two dimensions, and others are broadcasted
                # Before squeezing we have an intermediate shape of (None, 1152, 10, 1, 1)
                b += agreement
        return v

    @tf.function
    def regenerate_image(self, inputs):
        with tf.name_scope("Reconstruction") as scope:
            v_ = tf.reshape(inputs, [-1,
                                     self.no_of_secondary_capsules * self.secondary_capsule_vector])  # v_.shape: (None, 160)
            reconstructed_image = self.dense_1(v_)  # reconstructed_image.shape: (None, 512)
            reconstructed_image = self.dense_2(reconstructed_image)  # reconstructed_image.shape: (None, 1024)
            reconstructed_image = self.dense_3(reconstructed_image)  # reconstructed_image.shape: (None, 784)
        return reconstructed_image